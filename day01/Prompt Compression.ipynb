{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T08:58:20.291383Z",
     "start_time": "2024-07-29T08:58:18.424924Z"
    }
   },
   "source": "!pip install llmlingua",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmlingua\r\n",
      "  Obtaining dependency information for llmlingua from https://files.pythonhosted.org/packages/6e/3e/221fe46a3338f2babdb2082ee42df88fcaa8ea0e639e832cbb1b93c5923a/llmlingua-0.2.2-py3-none-any.whl.metadata\r\n",
      "  Downloading llmlingua-0.2.2-py3-none-any.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: transformers>=4.26.0 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (4.38.0)\r\n",
      "Requirement already satisfied: accelerate in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (0.25.0)\r\n",
      "Requirement already satisfied: torch in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (2.1.1)\r\n",
      "Requirement already satisfied: tiktoken in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (0.6.0)\r\n",
      "Requirement already satisfied: nltk in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (3.8.1)\r\n",
      "Requirement already satisfied: numpy in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from llmlingua) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (3.15.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.24.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (2024.7.24)\r\n",
      "Requirement already satisfied: requests in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (4.66.1)\r\n",
      "Requirement already satisfied: psutil in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from accelerate->llmlingua) (6.0.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from torch->llmlingua) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from torch->llmlingua) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from torch->llmlingua) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from torch->llmlingua) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from torch->llmlingua) (2024.3.1)\r\n",
      "Requirement already satisfied: click in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from nltk->llmlingua) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from nltk->llmlingua) (1.4.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (2024.7.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from jinja2->torch->llmlingua) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from sympy->torch->llmlingua) (1.3.0)\r\n",
      "Downloading llmlingua-0.2.2-py3-none-any.whl (30 kB)\r\n",
      "Installing collected packages: llmlingua\r\n",
      "Successfully installed llmlingua-0.2.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T08:59:33.984873Z",
     "start_time": "2024-07-29T08:58:32.271490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "# llm_lingua = PromptCompressor(\n",
    "#     model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "#     use_llmlingua2=True, # Whether to use llmlingua-2\n",
    "# )\n",
    "\n",
    "## Use LLMLingua-2-small model\n",
    "llm_lingua = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "    use_llmlingua2=True, # Whether to use llmlingua-2\n",
    ")"
   ],
   "id": "8daad519180c5941",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whale/work/whale.so/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/whale/work/whale.so/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/875 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d5cdc18bd874743909b1498c834d509"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be3a7b9854d14ae0a65a5d4cc8a18024"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "528a216b51a44fd4a8af190d1f3df25d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b8f6d0afeae463797f17abfccd62c0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc83424e79e74d06b472e9e56dd174db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whale/work/whale.so/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/709M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12f4786bfb224dd2bd5ef76eca738671"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllmlingua\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PromptCompressor\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# llm_lingua = PromptCompressor(\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#     model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#     use_llmlingua2=True, # Whether to use llmlingua-2\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m## Use LLMLingua-2-small model\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m llm_lingua \u001B[38;5;241m=\u001B[39m \u001B[43mPromptCompressor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmicrosoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_llmlingua2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Whether to use llmlingua-2\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:89\u001B[0m, in \u001B[0;36mPromptCompressor.__init__\u001B[0;34m(self, model_name, device_map, model_config, open_api_config, use_llmlingua2, llmlingua2_config)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprefix_bos_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moai_tokenizer \u001B[38;5;241m=\u001B[39m tiktoken\u001B[38;5;241m.\u001B[39mencoding_for_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 89\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_llmlingua2:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_llmlingua2(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mllmlingua2_config)\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:140\u001B[0m, in \u001B[0;36mPromptCompressor.load_model\u001B[0;34m(self, model_name, device_map, model_config)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    135\u001B[0m     device_map\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(key \u001B[38;5;129;01min\u001B[39;00m device_map \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    138\u001B[0m )\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map:\n\u001B[0;32m--> 140\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL_CLASS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch_dtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     model \u001B[38;5;241m=\u001B[39m MODEL_CLASS\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    152\u001B[0m         model_name,\n\u001B[1;32m    153\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_config,\n\u001B[1;32m    157\u001B[0m     )\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    560\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    565\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    567\u001B[0m )\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:3502\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3493\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3494\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   3495\u001B[0m     (\n\u001B[1;32m   3496\u001B[0m         model,\n\u001B[1;32m   3497\u001B[0m         missing_keys,\n\u001B[1;32m   3498\u001B[0m         unexpected_keys,\n\u001B[1;32m   3499\u001B[0m         mismatched_keys,\n\u001B[1;32m   3500\u001B[0m         offload_index,\n\u001B[1;32m   3501\u001B[0m         error_msgs,\n\u001B[0;32m-> 3502\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[1;32m   3506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3509\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3510\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3514\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3515\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3518\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3520\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   3521\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:3926\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001B[0m\n\u001B[1;32m   3924\u001B[0m                     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(model, param, key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, state_dict)\n\u001B[1;32m   3925\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3926\u001B[0m         new_error_msgs, offload_index, state_dict_index \u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3927\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3928\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3929\u001B[0m \u001B[43m            \u001B[49m\u001B[43mloaded_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3930\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstart_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3931\u001B[0m \u001B[43m            \u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3932\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3933\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3934\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3935\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3936\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3937\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3938\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3939\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_safetensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3940\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3941\u001B[0m \u001B[43m            \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3942\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3943\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_error_msgs\n\u001B[1;32m   3944\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/transformers/modeling_utils.py:805\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001B[0m\n\u001B[1;32m    798\u001B[0m     state_dict_index \u001B[38;5;241m=\u001B[39m offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\n\u001B[1;32m    799\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m    800\u001B[0m     hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m hf_quantizer\u001B[38;5;241m.\u001B[39mrequires_parameters_quantization)\n\u001B[1;32m    802\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m hf_quantizer\u001B[38;5;241m.\u001B[39mcheck_quantized_param(model, param, param_name, state_dict))\n\u001B[1;32m    803\u001B[0m ):\n\u001B[1;32m    804\u001B[0m     \u001B[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001B[39;00m\n\u001B[0;32m--> 805\u001B[0m     \u001B[43mset_module_tensor_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mset_module_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    807\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/accelerate/utils/modeling.py:317\u001B[0m, in \u001B[0;36mset_module_tensor_to_device\u001B[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001B[0m\n\u001B[1;32m    315\u001B[0m             module\u001B[38;5;241m.\u001B[39m_parameters[tensor_name] \u001B[38;5;241m=\u001B[39m param_cls(new_value, requires_grad\u001B[38;5;241m=\u001B[39mold_value\u001B[38;5;241m.\u001B[39mrequires_grad)\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 317\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m \u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(value, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "File \u001B[0;32m~/work/whale.so/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m     )\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"\"\"Madam Court, could you please read docket 1239? Certainly. Docket 1239. The Committee on Government Operations, to which was referred on December 1st, 2021, docket number 1239 message an order authorizing the creation of a sheltered market program in conformity with the requirements of general laws. Chapter 30 B Section 18. This authorization applies to contracts for goods, professional services and support services. This authorization is for no more than six contracts, which must be awarded by June 30th, 2022. This sheltered market program shall be available for disadvantaged, minority and women only vendors, for whom there is a demonstrated substantial disparity in the city's 2020 disparities. Study submits a report recommending the order ought to pass. Thank you so much, Madam Clerk. The Chair recognizes Councilor Edwards, chair of the committee. Councilor Edwards. You have the floor. This is this is actually a matter, I believe, sponsored by the. Mayor in Cannes. In conformance with the recommendations from the disparity study and making sure that we opt in to this this pilot program under mass general laws 30 Section 18. Again, it's really just following the recommendations of an already studied issue, which which demonstrates a disparity between minority contractors or women contractors receiving contracts in the city of Boston. So this would allow for us to shepherd and move these six contracts to those already designated groups who have a disadvantage. And I think it's. Really fulfilling a promise. Of making sure that we go through and make sure all aspects of the city government, including the financial benefits, are accessible to people in the city of Boston. I recommend that this pass and I hope that my colleagues will vote for it. Thank you. Thank you so much. Councilor Edward seeks acceptance of the committee report and passage of Docket 1239. Madam Court, could you please call the roll? Certainly. Docket 1239. Councilor Arroyo. Yes. Councilor Arroyo. Yes. Councilor Baker. Councilor Baker. Councilor. Councilor Barker. Council Braden. Councilor Braden. Councilor Campbell. Councilor Campbell. Yes. Councilor Edwards. Yes. Councilor Sabby. George. Councilor Sabby. George. He has Councilor Flaherty. Councilor Flaherty as Councilor Flynn. Councilor Flynn. Yes. Councilor Jane. Yes. Councilor Janey. As Councilor me here. Councilor me here as Councilor Murphy. Councilor Murphy. Yes. And Councilor O'Malley. Yes. Councilor O'Malley. Yes. Madam President, do I get number 1239 has received unanimous vote. Thank you so much. Dockett 1239 has passed and now we will move on to matters recently heard for possible action. Madam Clerk, if you could please read docket 0863. Certainly Docket 0863 order for hearing to discuss pest control and illegal dumping in the city of Boston.\"\"\"\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, rate=0.33, force_tokens = ['\\n', '?'])"
   ],
   "id": "65583e46b389a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('original prompt length: ', len(prompt))\n",
    "print('compressed prompt length: ',len(compressed_prompt['compressed_prompt']))\n",
    "compressed_prompt['compressed_prompt']"
   ],
   "id": "b552a334db6bc3dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
