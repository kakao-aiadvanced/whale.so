{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "의존성 및 키 세팅",
   "id": "a036db79bdf99843"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:37.170901Z",
     "start_time": "2024-07-29T23:49:34.751351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchainhub langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community"
   ],
   "id": "a426403ee3806e66",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:38.317656Z",
     "start_time": "2024-07-29T23:49:38.311375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "key_path = '../openai_key.txt'\n",
    "with open(key_path, 'r') as file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = file.read().strip()"
   ],
   "id": "f6ea299f259700b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T23:49:38.990517Z",
     "start_time": "2024-07-29T23:49:38.970357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "메세지 생성",
   "id": "2fcda0aa761bd378"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:12:14.506911Z",
     "start_time": "2024-07-30T00:12:13.798359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 프롬프트를 가져오거나 등록할 수 있음\n",
    "from langchain import hub\n",
    "\n",
    "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 프롬프트 템플릿 채우는 예시\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "print(prompt)\n",
    "print(example_messages)\n"
   ],
   "id": "ddac117994fefd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n",
      "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:00:50.373811Z",
     "start_time": "2024-07-30T00:00:50.292026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ],
   "id": "b1881e86265a87d5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:06:00.039835Z",
     "start_time": "2024-07-30T00:05:57.677636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 청크 설정\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# 설정된 청크로 분리\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# 청크된 문서로 Chroma 객체 생성\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ],
   "id": "27e5e465e1c3653d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "id": "c1c8affb21b7cb48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1d41ab649c1cbe75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:33:20.171262Z",
     "start_time": "2024-07-30T00:33:18.552630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 3개의 블로그 포스팅 본문을 Load: WebBaseLoader 활용\n",
    "# https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/\n",
    "\n",
    "# 동시에 여러 웹사이트에서 문서를 로드할 수 있음\n",
    "%pip install --upgrade --quiet  nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "loader.requests_per_second = 1\n",
    "documents = loader.aload()\n",
    "\n",
    "print(docs[0].page_content[:100])"
   ],
   "id": "d5a04fcb8637e218",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00, 27.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T00:34:02.821974Z",
     "start_time": "2024-07-30T00:33:53.684033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 불러온 본문을 Split (Chunking) : recursive text splitter 활용\n",
    "!pip install faiss-cpu\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n"
   ],
   "id": "4b1a2de22e4a8be7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\r\n",
      "  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/c2/2a/e72ebe364754fb110cfd802aec92c2a1bb3590f55d6c1910b3c6fe3c6d18/faiss_cpu-1.8.0.post1-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /Users/whale/work/whale.so/venv/lib/python3.9/site-packages (from faiss-cpu) (24.1)\r\n",
      "Downloading faiss_cpu-1.8.0.post1-cp39-cp39-macosx_11_0_arm64.whl (6.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m21.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: faiss-cpu\r\n",
      "Successfully installed faiss-cpu-1.8.0.post1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:16:23.791653Z",
     "start_time": "2024-07-30T07:16:15.581635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Chunks 를 임베딩하여 Vector store 저장: openai, chroma 사용\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "bb6b951912f51e8f",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:14:44.932005Z",
     "start_time": "2024-07-30T07:14:44.502139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"agent memory\"\n",
    "docs = retriever.invoke(query)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)"
   ],
   "id": "593e3b5c95559d15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "page_content='Agent System Overview#' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8bbd6c793bf9611e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:41:22.168292Z",
     "start_time": "2024-07-30T06:41:19.697764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "retriever1 = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "print(retriever1.invoke(query))\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "retriever2 = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "print(retriever2.invoke(query))\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "retriever3 = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "print(retriever3.invoke(query))\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "retriever4 = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "print(retriever4.invoke(query))\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "retriever5 = vectorstore.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title': 'GPT-4 Technical Report'}}\n",
    ")\n",
    "print(retriever5.invoke(query))\n"
   ],
   "id": "b8bf96de5cfd0b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Agent System Overview#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='in the plan). Then reflections are added into the agent’s working memory, up to three, to be used'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.', 'language': 'en'}, page_content='Definition: Determine the speaker of the dialogue, \"agent\" or \"customer\".'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='environment many times and in each episode the agent gets a little better, AD concatenates this')]\n",
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Agent System Overview#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Memory'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.', 'language': 'en'}, page_content='the global model behavior.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.', 'language': 'en'}, page_content='APE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/whale/work/whale.so/venv/lib/python3.9/site-packages/langchain_core/vectorstores/base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}, page_content='Agent System Overview#')]\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:27:30.517455Z",
     "start_time": "2024-07-30T07:27:29.942300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. User query와 retrieved chunk 에 대해 relevance 가 있는지를 평가하는 시스템 프롬프트 작성: retrieval 퀄리티를 LLM 이 스스로 평가하도록 하고, 관련이 있으면 {‘relevance’: ‘yes’} 관련이 없으면 {‘relevance’: ‘no’} 라고 출력하도록 함. ( JsonOutputParser() 를 활용 )\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-document-relevance\")\n",
    "prompt"
   ],
   "id": "482a47c0fd75f9e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['input'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-document-relevance', 'lc_hub_commit_hash': '123af323ca4720fc195f1d6966ba25fa67348c351972be2088faf2b22f2de056'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a grader assessing relevance of a retrieved document to a user question.\\n\\nIf the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n\\nIt does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n\\nGive a binary score 1 or 0 score, where 1 means that the document is relevant to the question.', template_format='mustache')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Retrieved documents: {{input.documents}} \\n\\nUser question: {{input.question}}', template_format='mustache'))], schema_={'type': 'object', 'title': 'Score_criteria', 'required': ['Score', 'Explaination'], 'properties': {'Score': {'type': 'integer', 'description': 'Are the retrieved documents relevant to the user question?'}, 'Explaination': {'type': 'string', 'description': 'Explain your reasoning for the score:'}}, 'description': 'Score the retrieved documents for whether they are relevant to the user question.'})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:13:21.835557Z",
     "start_time": "2024-07-30T07:13:21.828065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "format_instructions = prompt.invoke(\n",
    "    {\"input\": {\n",
    "        \"question\": query,\n",
    "        \"documents\": docs[0]\n",
    "    }}\n",
    ").to_messages()\n",
    "\n",
    "format_instructions"
   ],
   "id": "dcde08e982640b2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a grader assessing relevance of a retrieved document to a user question.\\n\\nIf the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n\\nIt does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n\\nGive a binary score 1 or 0 score, where 1 means that the document is relevant to the question.'),\n",
       " HumanMessage(content=\"Retrieved documents: page_content='Agent System Overview#' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': &quot;LLM Powered Autonomous Agents | Lil'Log&quot;, 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'} \\n\\nUser question: agent memory\")]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:28:24.453637Z",
     "start_time": "2024-07-30T07:28:23.351209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain = (\n",
    "        {\"input\": {\"question\": RunnablePassthrough(),\"documents\": retriever | format_docs}}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(joke_query)"
   ],
   "id": "3e4980a63a440df8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Score: 0'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f089ed65fa1043f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
