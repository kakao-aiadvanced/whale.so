{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:35:52.811116Z",
     "start_time": "2024-07-30T07:35:47.933291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "# 사전 세팅\n",
    "%pip install langchain langchainhub langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community\n",
    "%pip install --upgrade --quiet  nest_asyncio\n",
    "%pip install faiss-cpu\n",
    "\n",
    "import os\n",
    "\n",
    "key_path = '../openai_key.txt'\n",
    "with open(key_path, 'r') as file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = file.read().strip()"
   ],
   "id": "588f502b1cba168d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:39:23.096117Z",
     "start_time": "2024-07-30T07:39:19.775632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0. RAG 벡터 생성\n",
    "import nest_asyncio\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "loader.requests_per_second = 1\n",
    "documents = loader.aload()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "dfdab0c16d42cbea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:43:01.985889Z",
     "start_time": "2024-07-30T07:43:01.977148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 쿼리 입력\n",
    "query = \"What is the capital of Korea?\""
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:44:49.411436Z",
     "start_time": "2024-07-30T07:44:47.522906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Relevance Checker\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def get_relevance_score(query: str):\n",
    "    prompt = hub.pull(\"rlm/rag-document-relevance\")\n",
    "    chain = (\n",
    "            {\"input\": {\"question\": RunnablePassthrough(), \"documents\": retriever | format_docs}}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)"
   ],
   "id": "d8a67ad681e5cabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:29:34.712808Z",
     "start_time": "2024-07-30T08:29:33.148935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if (get_relevance_score(query) == \"0\"):\n",
    "    print(\"No\")\n",
    "\n",
    "\n",
    "def generate_answer(query: str):\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)\n",
    "\n"
   ],
   "id": "45c809b5a439cc5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:29:35.550348Z",
     "start_time": "2024-07-30T08:29:35.544344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check hallucination\n",
    "def check_hallucination(query: str, output: str):\n",
    "    generate_answer = lambda q: output  # Placeholder for the actual generation function\n",
    "    prompt = hub.pull(\"rlm/rag-answer-hallucination\")\n",
    "    chain = (\n",
    "            {\"input\": {\"documents\": retriever | format_docs}, \"output\": generate_answer}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)\n"
   ],
   "id": "a86577e28e9a3200",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:29:48.493004Z",
     "start_time": "2024-07-30T08:29:37.226646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run(query: str):\n",
    "    relevance_score = get_relevance_score(query)\n",
    "    if (relevance_score == \"0\"):\n",
    "        return \"No\"\n",
    "\n",
    "    answer = generate_answer(query)\n",
    "\n",
    "    if (check_hallucination(query, answer) == \"1\"):\n",
    "        return generate_answer(query)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(run(\"RAG 에 대한 저자의 생각은 무엇인가?\"))\n",
    "# 저자는 RAG 스타일이 여러 확률 모델을 결합하여 답변 후보를 재정렬하는 데 효과적이라는 점을 강조한다. 실험 결과에 따르면, RAG는 다른 방법들에 비해 성능이 떨어지며, 특히 PoE 방식이 더 우수한 성과를 보인다. 또한, 질문과 증거를 기반으로 한 모델의 성능 차이를 통해 모델의 내부 지식과 맥락 정보 간의 불일치를 지적하고 있다.\n",
    "\n",
    "print(run(\"오늘 점심 메뉴는 무엇인가?\"))\n",
    "# No"
   ],
   "id": "431c5f4123d10c12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저자는 RAG 스타일이 여러 확률 모델을 결합하여 답변 후보를 재정렬하는 데 효과적이라는 점을 강조한다. 실험 결과에 따르면, RAG는 다른 방법들에 비해 성능이 떨어지며, 특히 PoE 방식이 더 우수한 성과를 보인다. 또한, 질문과 증거를 기반으로 한 모델의 성능 차이를 통해 모델의 내부 지식과 맥락 정보 간의 불일치를 지적하고 있다.\n",
      "No\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:16:26.382732Z",
     "start_time": "2024-07-30T08:16:25.573489Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "662118cf0451769d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input', 'output'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-answer-hallucination', 'lc_hub_commit_hash': 'a88d01cb864e906293aae38575a85627b3b932d6ba2210de4bd69cea9bfd99ab'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\\nGive a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.', template_format='mustache')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'output'], template='Facts: {{input.documents}} \\n\\nLLM generation: {{output}}', template_format='mustache'))] schema_={'type': 'object', 'title': 'Criteria', 'required': ['Score', 'Explanation'], 'properties': {'Score': {'type': 'integer', 'description': 'Is the LLM generation grounded in the Facts?'}, 'Explanation': {'type': 'string', 'description': 'Explain your reasoning for the score:'}}, 'description': 'Score the LLM generation for whether it is grounded in the Facts.'}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46857f1c435f5db7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
