{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:35:52.811116Z",
     "start_time": "2024-07-30T07:35:47.933291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "# 사전 세팅\n",
    "%pip install langchain langchainhub langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community\n",
    "%pip install --upgrade --quiet  nest_asyncio\n",
    "%pip install faiss-cpu\n",
    "\n",
    "import os\n",
    "\n",
    "key_path = '../openai_key.txt'\n",
    "with open(key_path, 'r') as file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = file.read().strip()"
   ],
   "id": "588f502b1cba168d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:39:23.096117Z",
     "start_time": "2024-07-30T07:39:19.775632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0. RAG 벡터 생성\n",
    "import nest_asyncio\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "loader.requests_per_second = 1\n",
    "documents = loader.aload()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "dfdab0c16d42cbea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:43:01.985889Z",
     "start_time": "2024-07-30T07:43:01.977148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 쿼리 입력\n",
    "query = \"What is the capital of Korea?\""
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:44:49.411436Z",
     "start_time": "2024-07-30T07:44:47.522906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Relevance Checker\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def get_relevance_score(query: str):\n",
    "    prompt = hub.pull(\"rlm/rag-document-relevance\")\n",
    "    chain = (\n",
    "            {\"input\": {\"question\": RunnablePassthrough(), \"documents\": retriever | format_docs}}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)"
   ],
   "id": "d8a67ad681e5cabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:19:38.706552Z",
     "start_time": "2024-07-30T08:19:35.249014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if (get_relevance_score(query) == \"0\"):\n",
    "    print(\"No\")\n",
    "\n",
    "\n",
    "def generate_answer(query: str):\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)\n",
    "\n"
   ],
   "id": "45c809b5a439cc5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n",
      "content='The capital of Korea is Seoul.' response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 500, 'total_tokens': 507}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-fb00077f-8196-437d-a4b0-dc2d88c7dac5-0' usage_metadata={'input_tokens': 500, 'output_tokens': 7, 'total_tokens': 507}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:22:55.022680Z",
     "start_time": "2024-07-30T08:22:53.154114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check hallucination\n",
    "def check_hallucination(query: str, output: str):\n",
    "    prompt = hub.pull(\"rlm/rag-answer-hallucination\")\n",
    "    chain = (\n",
    "            {\"input\": {\"documents\": retriever | format_docs}, \"output\": {}}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(query)\n",
    "\n",
    "\n",
    "check_hallucination(query, \"output\")"
   ],
   "id": "a86577e28e9a3200",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:22:08.939208Z",
     "start_time": "2024-07-30T08:22:00.439545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run(query: str):\n",
    "    relevance_score = get_relevance_score(query)\n",
    "    if (relevance_score == \"0\"):\n",
    "        return \"No\"\n",
    "\n",
    "    answer = generate_answer(query)\n",
    "\n",
    "    if (check_hallucination(query, answer) == \"1\"):\n",
    "        return generate_answer(query)\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(run(\"RAG 에 대한 저자의 생각은 무엇인가?\"))\n",
    "print(run(\"오늘 점심 메뉴는 무엇인가?\"))"
   ],
   "id": "431c5f4123d10c12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='저자는 RAG 방식이 다양한 확률 기반 접근 방식 중에서 상대적으로 덜 효과적이라는 결론을 내립니다. 실험에 따르면, PoE가 RAG보다 더 높은 성능을 보이며, RAG의 개별 확률이 다른 방식에 비해 덜 정보적임을 시사합니다. 또한, 모델이 최신 정보에 접근할 수 있음에도 불구하고, 최신 질문에 대한 성능이 떨어진다는 점도 지적하고 있습니다.' response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 831, 'total_tokens': 933}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-d5f4eb8e-b67c-48f1-8bda-1d126a22619d-0' usage_metadata={'input_tokens': 831, 'output_tokens': 102, 'total_tokens': 933}\n",
      "No\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:16:26.382732Z",
     "start_time": "2024-07-30T08:16:25.573489Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "662118cf0451769d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input', 'output'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-answer-hallucination', 'lc_hub_commit_hash': 'a88d01cb864e906293aae38575a85627b3b932d6ba2210de4bd69cea9bfd99ab'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\\nGive a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.', template_format='mustache')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'output'], template='Facts: {{input.documents}} \\n\\nLLM generation: {{output}}', template_format='mustache'))] schema_={'type': 'object', 'title': 'Criteria', 'required': ['Score', 'Explanation'], 'properties': {'Score': {'type': 'integer', 'description': 'Is the LLM generation grounded in the Facts?'}, 'Explanation': {'type': 'string', 'description': 'Explain your reasoning for the score:'}}, 'description': 'Score the LLM generation for whether it is grounded in the Facts.'}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46857f1c435f5db7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
